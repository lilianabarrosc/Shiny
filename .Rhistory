respuesta <- crossValidation(model_lm, theta.predict, airquality[,1], airquality[,2:6])
respuesta <- crossValidation(model_lm, theta.predict, airquality[,1], airquality[,2:5])
predictors <- function(dataSet, x, y){
if(is.null(x)){
return (dataSet[, !names(dataSet) %in% y])
}else{
predictors <- dataSet[, !names(dataSet) %in% y]
return(predictors[,x])
}
}
y <- "Ozone"
x <- NULL
predictors(airquality, x, y)
result <- predictors(airquality, x, y)
View(result)
x[1] <- "Wind"
x[2] <- "Temp"
result2 <- predictors(airquality, x, y)
View(result2)
predictors <- function(dataSet, x, y){
if(is.null(x)){
return (dataSet[, !names(dataSet) %in% y])
}else{
predictors <- dataSet[, !names(dataSet) %in% y]
return(predictors[,!names(predictors) %in% x])
}
}
result2 <- predictors(airquality, x, y)
View(result2)
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
paste("hola", names(iris))
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
ui
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
}else{"FALSE"}
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
anyNA(airquality)
runApp('GitHub/Shiny')
airquality[,-c(1,2)]
iris.x <- iris[,1:4]
diffValues <- calculateDiff(iris.x)
columnsNoise <- getColumnsNoise(diffValues, 0.7)
View(columnsNoise)
iris.x[,-columnsNoise[1]]
iris.x[,-columnsNoise[,1]]
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
lista <- type_normalization()
names(lista)
names(lista[n0])
names(lista[[n0]])
names(lista[["n0"]])
names(lista["n0"])
lista["n0"]
lista[1]
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
}}
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
library(car)
cars
library('plsdepot')
shiny::runApp('GitHub/Shiny')
str(mtcars)
write.csv(mtcars, file = "mtcars.csv")
set.seed(2)
x=matrix(rnorm(50*2), ncol=2) #matriz con 50 observaciones y dos variables
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
hc.complete=hclust(dist(x), method="complete")
hc.average=hclust(dist(x), method="average")
hc.single=hclust(dist(x), method="single")
cutree(hc.complete, 4)
cutree(hc.average, 2)
cutree(hc.single, 2)
cutree(hc.single, 4)
xsc=scale(x)
plot(hclust(dist(xsc), method="complete"), main="Hierarchical Clustering with Scaled Features")
x=matrix(rnorm(30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method="complete"), main="Complete Linkage with Correlation-Based Distance", xlab="", sub="")
library(ISLR)
nci.labs=NCI60$labs
nci.data=NCI60$data
names(NCI60)
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
pr.out=prcomp(nci.data, scale=TRUE)
Cols=function(vec){
cols=rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z2")
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z3")
summary(pr.out)
plot(pr.out)
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve,  type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
p1 <- elbowPlot(pr.out)
summary(pr.out)$importance[2,]
summary(pr.out)$importance[3,]
sd.data=scale(nci.data)
par(mfrow=c(1,3))
data.dist=dist(sd.data)
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
hc.out=hclust(dist(sd.data))
hc.clusters=cutree(hc.out,4)
hc.clusters
table(nci.labs)
table(hc.clusters,nci.labs)
par(mfrow=c(1,1))
plot(hc.out, labels=nci.labs)
abline(h=139, col="red")
hc.out
clusters <- hclust(dist(iris[, 1:2]))
plot(clusters)
clusterCut <- cutree(clusters, 3)
table(clusterCut, iris$Species)
clusters <- hclust(dist(iris[, 1:4]))
names(iris)
clusters <- hclust(dist(iris[, 1:4]))
plot(clusters)
clusterCut <- cutree(clusters, 2)
table(clusterCut, iris$Species)
clusterCut <- cutree(clusters, 1)
table(clusterCut, iris$Species)
clusterCut <- cutree(clusters, 3)
table(clusterCut, iris$Species)
clusterCut <- cutree(clusters, 4)
table(clusterCut, iris$Species)
clusters <- hclust(dist(iris[, 1:4]), method = 'average')
plot(clusters)
clusterCut <- cutree(clusters, 2)
table(clusterCut, iris$Species)
clusterCut <- cutree(clusters, 3)
table(clusterCut, iris$Species)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) +
geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) +
scale_color_manual(values = c('black', 'red', 'green'))
library(DMwR)
install.packages(DMwR)
install.packages('DMwR')
library(DMwR)
str(algae)
library(DMwR)
head(algae)
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
summary(algae)
algae[!complete.cases(algae),]
nrow(algae[!complete.cases(algae),])
library(shiny)
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
library(DMwR)
data(algae)
algae[!complete.cases(algae),]
nrow(algae[!complete.cases(algae),])
algae <- na.omit(algae)
apply(algae,1,function(x) sum(is.na(x)))
algae <- na.omit(algae)
algae <- algae[-c(62,199),]
nrow(algae[!complete.cases(algae),])
algae <- na.omit(algae)
data(algae)
nrow(algae[!complete.cases(algae),])
algae <- na.omit(algae)
nrow(algae[!complete.cases(algae),])
apply(algae,1,function(x) sum(is.na(x)))
data(algae)
algae <- algae[-manyNAs(algae),]
algae <- centralImputation(algae)
nrow(algae[!complete.cases(algae),])
data(algae)
algae <- algae[-manyNAs(algae),]
nrow(algae[!complete.cases(algae),])
algae <- centralImputation(algae)
nrow(algae[!complete.cases(algae),])
data(algae)
nrow(algae[!complete.cases(algae),])
algae <- na.omit(algae)
str(algae)
library(DMwR)
data(algae)
algae <- algae[-manyNAs(algae),]
algae <- knnImputation(algae,k=10)
data(algae)
algae <- algae[-manyNAs(algae), ]
clean.algae <- knnImputation(algae, k = 10)
lm.a1 <- lm(a1 ~ .,data=clean.algae[,1:12])
anova(lm.a1)
lm2.a1 <- update(lm.a1, . ~ . - season)
anova(lm.a1,lm2.a1)
final.lm <- step(lm.a1)
summary(final.lm)
srt(algae)
str(algae)
View(algae)
library(ISLR)
set.seed(1)
train=sample(392,196) #vector con pos del data set aleatorias (50% del data set)
set.seed(1)
train=sample(392,196)
train
dim(train)
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
summary(lm.fit)
predict(lm.fit,Auto)
prediccion <- predict(lm.fit,Auto)
response.variable <- Auto[,"mpg"]
write.csv(Auto, file = "Auto.csv")
mean((mpg-predict(lm.fit,Auto))[-train]^2)
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
pr.out=prcomp(USArrests, scale=TRUE) #scale proporciona una desiacion estandar
names(pr.out)
pr.out$center #media
pr.out$scale #desviacion estandar
pr.out$rotation #componentes principales
pr.var=pr.out$sdev^2 #varianza
pve=pr.var/sum(pr.var) #variance explained
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
pve
par(mfrow=c(1,2))
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
write.csv(USArrests, file = "USArrests.csv")
library(ISLR) #paquete contenedor del data set Hitters
library(glmnet) #paquete para utilizar ridge
names(Hitters)
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
x = model.matrix(Salary~.,Hitters )[,-1]
y = Hitters$Salary
grid=10^seq(10,-2,length=100) #aplicaremos esto para valores de 10^10 a 10^-2 de lambda
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # alpha = 0 <- ridge alpha = 1 <- lasso
dim(coef(ridge.mod)) #matriz de 20x100 20 predictores y 100 lambda
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) #se elige al azar nrow(x)/2 elementos del conjunto 1:nrow(x)
test=(-train)
y.test=y[test] #se obtienen los valores y para los indices test.
cv.out=cv.glmnet(x[train,],y[train],alpha=0) #cv.glmnet incorpora la validacion cruzada, por defecto 10
plot(cv.out)
plot(cv.out)
bestlam=cv.out$lambda.min #obtenemos el lambda mas pequeño = 212
bestlam
set.seed(1) #aplicamos validacion cruzada al azar
cv.out=cv.glmnet(x[train,],y[train],alpha=0) #cv.glmnet incorpora la validacion cruzada, por defecto 10
plot(cv.out)
bestlam=cv.out$lambda.min #obtenemos el lambda mas pequeño = 212
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
out=glmnet(x,y,alpha=0, lambda= bestlam)
out
summary(out)
write.csv(Hitters, file = "Hitters.csv")
data("Hitters")
write.csv(Hitters, file = "Hitters.csv")
dim(Hitters)
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
library(pls)
set.seed(1)
pls.fit=plsr(Salary~., data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
library('RegressionLibs')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
library(ISLR) #paquete contenedor del data set Hitters
library(glmnet) #paquete para utilizar ridge
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
x = model.matrix(Salary~.,Hitters )[,-1]
y = Hitters$Salary
bestlam=cv.out$lambda.min #obtenemos el lambda mas pequeño = 212
cv.out=cv.glmnet(x[train,],y[train],alpha=0) #cv.glmnet incorpora la validacion cruzada, por defecto 10
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) #se elige al azar nrow(x)/2 elementos del conjunto 1:nrow(x)
test=(-train)
y.test=y[test]
set.seed(1) #aplicamos validacion cruzada al azar
cv.out=cv.glmnet(x[train,],y[train],alpha=0) #cv.glmnet incorpora la validacion cruzada, por defecto 10
bestlam=cv.out$lambda.min #obtenemos el lambda mas pequeño = 212
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
library(glmnet) #paquete para utilizar ridge
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
out=glmnet(x,y,alpha=0, lambda= bestlam)
ridge.pred=predict(out,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
rmse(x, y)
mean((ridge.pred-y.test)^2)
out=glmnet(x[train,],y[train],alpha=0, lambda= bestlam)
ridge.pred=predict(out,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
rmse(x[train,],y[train])
rmse(model.matrix(x[test,]),as.data.frame(y[test]))
ridge.fitted <- cbind(1,x[test,])%*%coef(out)
rmse(model.matrix(ridge.fitted),as.data.frame(y[test]))
predict(out,type="coefficients",s=bestlam)[1:20,]
ridge.fitted
ridge.fitted <- cbind(1,x[train,])%*%coef(out)
rmse(model.matrix(ridge.fitted),as.data.frame(y[train]))
ridge.fitted <- cbind(1,x[test,])%*%coef(out)
rmse(model.matrix(ridge.fitted),as.data.frame(y.test))
mean((ridge.pred-y.test)^2)
runApp('GitHub/Shiny')
ridge.fitted <- cbind(1,x[test,])%*%coef(out)
rmse(model.matrix(ridge.fitted),as.data.frame(y.test))
ridge.fitted <- cbind(1,x[test,])%*%coef(out)
rmse(as.matrix(ridge.fitted),as.data.frame(y.test))
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
fators <- sapply(iris, is.factor)
n <- lapply(iris[,fators], as.numeric)
n <- sapply(iris[,fators], as.numeric)
runApp('GitHub/Shiny')
n <- sapply(iris[,fators], as.numeric)
nums <- sapply(iris, is.numeric)
a <- cbind(iris[,nums], n)
View(a)
runApp('GitHub/Shiny')
fators <- sapply(iris, is.factor) #detecta las variables nominales
n <- sapply(iris[,fators], as.numeric) #las tranforma a datos numericos
nums <- sapply(iris, is.numeric)
a <- cbind(iris[,nums], n)
View(a)
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
View(a)
runApp('GitHub/Shiny')
names(iris)
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
drv <- dbDriver("PostgreSQL")
dbListConnections(drv)
dbGetInfo(drv)
con <- dbConnect(drv, host='localhost', port='5432', dbname='Guinia_bd',
user='postgres', password='liliana10')
sql <- "delete from data_set
where id='1463440592_e02aa87e9e85e85c3e0fe8832a440a9a.csv'"
rs <- dbSendQuery(con, sql)
dbHasCompleted(rs) #retorna si la consulta fue exitosa o no
j <- 1
for(i in 1:3){
list.data$data_setsID[[i]] <- as.character(paste("hola",j))
j <- j+1
}
list.data <- reactiveValues(data_sets = list("iris" = 1, "airquality" = 2, "sleep" = 3),
data_setsID = list(iris, airquality, sleep))
j <- 1
for(i in 1:3){
list.data$data_setsID[[i]] <- as.character(paste("hola",j))
j <- j+1
}
#Cerrar la conexion
dbDisconnect(con)
#liberar la conexion con el driver
dbUnloadDriver(drv)
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
shiny::runApp('GitHub/Shiny')
runApp('GitHub/Shiny')
setwd("~/GitHub/Shiny")
runApp()
library(rmarkdown)
runApp()
runApp()
